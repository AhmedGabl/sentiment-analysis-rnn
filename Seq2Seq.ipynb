{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedGabl/sentiment-analysis-rnn/blob/main/Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Introduction to Seq2Seq Models\n",
        "\n",
        "**Seq2Seq (Sequence-to-Sequence)** models are a class of machine learning models designed for tasks where both input and output are sequences. This architecture is commonly used in applications like:\n",
        "\n",
        "- **Machine Translation**: Translating sentences from one language to another.\n",
        "- **Text Summarization**: Generating summaries of long text.\n",
        "- **Speech-to-Text**: Converting speech into text.\n",
        "- **Question Answering**: Generating answers to questions based on context.\n",
        "\n",
        "A Seq2Seq model typically consists of two main components:\n",
        "\n",
        "1. **Encoder**: Encodes the input sequence into a context vector (fixed-size vector, typically using RNNs, LSTMs, or GRUs).\n",
        "2. **Decoder**: Decodes the context vector into the output sequence.\n",
        "\n",
        "#### Architecture Overview:\n",
        "\n",
        "1. **Encoder**:\n",
        "    - Takes the input sequence (such as a sentence) and processes it step-by-step.\n",
        "    - Each step updates a hidden state that captures the information from the sequence.\n",
        "\n",
        "2. **Decoder**:\n",
        "    - Uses the encoder's final hidden state as the context to generate the output sequence.\n",
        "    - It predicts the next word or token in the sequence one at a time.\n",
        "\n",
        "The Seq2Seq model can be trained using teacher forcing, where the true output sequence is fed as input to the decoder at each step during training.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Libraries & Setup\n",
        "\n",
        "To implement the Seq2Seq model, we'll use **TensorFlow 2.x** and **Keras** in a Google Colab notebook. Let's start by installing and importing the necessary libraries:\n",
        "\n",
        "```python\n",
        "# Install TensorFlow\n",
        "!pip install tensorflow\n",
        "\n",
        "# Import required libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Dataset Preparation\n",
        "\n",
        "For this notebook, we'll use a simple example of **English to French Translation** using a small dataset of parallel sentences.\n",
        "\n",
        "We'll first preprocess the data by:\n",
        "- Tokenizing text into sequences.\n",
        "- Padding sequences to ensure uniform length.\n",
        "- Creating training and testing datasets.\n",
        "\n",
        "#### Example Dataset:\n",
        "\n",
        "Here’s a small sample of parallel sentences for English and French:\n",
        "\n",
        "```plaintext\n",
        "English: \"Hello\"\n",
        "French: \"Bonjour\"\n",
        "\n",
        "English: \"How are you?\"\n",
        "French: \"Comment ça va?\"\n",
        "```\n",
        "\n",
        "We will create the following functions for dataset preparation.\n",
        "\n",
        "```python\n",
        "# Sample parallel sentences\n",
        "english_sentences = [\"Hello\", \"How are you?\", \"I am fine.\", \"Good morning\", \"Good night\"]\n",
        "french_sentences = [\"Bonjour\", \"Comment ça va?\", \"Je vais bien.\", \"Bonjour\", \"Bonne nuit\"]\n",
        "\n",
        "# Tokenization and Padding\n",
        "def tokenize_and_pad(texts, tokenizer=None, max_length=10):\n",
        "    if tokenizer is None:\n",
        "        tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "        tokenizer.fit_on_texts(texts)\n",
        "    \n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "    \n",
        "    return padded_sequences, tokenizer\n",
        "\n",
        "# Tokenizing and padding English and French texts\n",
        "english_padded, english_tokenizer = tokenize_and_pad(english_sentences)\n",
        "french_padded, french_tokenizer = tokenize_and_pad(french_sentences)\n",
        "```\n",
        "\n",
        "This function will tokenize the input and output sentences and ensure they are padded to the same length.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Building the Seq2Seq Model\n",
        "\n",
        "Now that the data is preprocessed, let's build the Seq2Seq model. For simplicity, we'll use **LSTM** cells for both the encoder and decoder.\n",
        "\n",
        "```python\n",
        "# Hyperparameters\n",
        "latent_dim = 256  # Latent dimension for LSTM\n",
        "max_encoder_seq_length = max(len(sentence.split()) for sentence in english_sentences)\n",
        "max_decoder_seq_length = max(len(sentence.split()) for sentence in french_sentences)\n",
        "num_encoder_tokens = len(english_tokenizer.word_index) + 1\n",
        "num_decoder_tokens = len(french_tokenizer.word_index) + 1\n",
        "\n",
        "# Encoder Model\n",
        "encoder_inputs = layers.Input(shape=(None,))\n",
        "encoder_embedding = layers.Embedding(input_dim=num_encoder_tokens, output_dim=latent_dim)(encoder_inputs)\n",
        "encoder_lstm, state_h, state_c = layers.LSTM(latent_dim, return_state=True)(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder Model\n",
        "decoder_inputs = layers.Input(shape=(None,))\n",
        "decoder_embedding = layers.Embedding(input_dim=num_decoder_tokens, output_dim=latent_dim)(decoder_inputs)\n",
        "decoder_lstm = layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_lstm_output, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = layers.Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_lstm_output)\n",
        "\n",
        "# Seq2Seq Model\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "```\n",
        "\n",
        "This is a basic Seq2Seq model:\n",
        "- **Encoder**: An LSTM layer to process the input.\n",
        "- **Decoder**: Another LSTM layer to process the output, using the encoder's final state.\n",
        "- **Dense Layer**: To output a probability distribution over the target vocabulary for each timestep in the sequence.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Training the Model\n",
        "\n",
        "Now, let's train the model. Since we are using **teacher forcing**, we need to provide the target sequence shifted by one timestep.\n",
        "\n",
        "```python\n",
        "# Preparing data for training\n",
        "decoder_input_data = french_padded[:, :-1]  # Remove last token\n",
        "decoder_target_data = french_padded[:, 1:]  # Remove first token\n",
        "\n",
        "# Train the model\n",
        "model.fit([english_padded, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
        "          batch_size=32, epochs=100, validation_split=0.2)\n",
        "```\n",
        "\n",
        "The target data is shifted by one timestep to ensure that the model learns to predict the next word in the sequence.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Inference: Decoding\n",
        "\n",
        "Once the model is trained, we need to decode new sentences by generating the output sequence one word at a time.\n",
        "\n",
        "```python\n",
        "# Inference models for prediction\n",
        "\n",
        "# Define an inference model for the encoder\n",
        "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "# Define an inference model for the decoder\n",
        "decoder_state_input_h = layers.Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = layers.Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm_output, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_lstm_output)\n",
        "decoder_model = keras.Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "# Function to decode a sequence\n",
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    target_seq = np.zeros((1, 1))  # Start with a dummy token\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "\n",
        "        # Get the most likely next token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = french_tokenizer.index_word[sampled_token_index]\n",
        "        decoded_sentence += ' ' + sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or end token\n",
        "        if sampled_token == '<end>' or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence and states\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "# Test decoding\n",
        "test_sentence = \"Hello\"\n",
        "test_sequence = english_tokenizer.texts_to_sequences([test_sentence])\n",
        "test_sequence = tf.keras.preprocessing.sequence.pad_sequences(test_sequence, maxlen=max_encoder_seq_length, padding='post')\n",
        "\n",
        "decoded_sentence = decode_sequence(test_sequence)\n",
        "print(f\"Input: {test_sentence}\")\n",
        "print(f\"Decoded: {decoded_sentence}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### 7. Conclusion\n",
        "\n",
        "This notebook provides a complete guide to building, training, and using a Seq2Seq model for sequence generation tasks. We've created a simple Seq2Seq architecture using LSTM units in TensorFlow/Keras and performed English-to-French translation on a toy dataset.\n",
        "\n",
        "You can further enhance this model by:\n",
        "- Increasing dataset size.\n",
        "- Experimenting with different architectures such as **GRU** or **Bidirectional LSTMs**.\n",
        "- Fine-tuning hyperparameters like latent dimension size and sequence length.\n",
        "\n",
        "Happy coding!"
      ],
      "metadata": {
        "id": "p5eo8TDkNHTL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQ4UbzR0Mv9s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Certainly! Let's dive deeper into more advanced concepts related to Seq2Seq models, including **delayed batching**, **gradient clipping**, and other techniques that can be used to improve training and performance in Seq2Seq models. These concepts are crucial for improving the efficiency and stability of training for more complex tasks.\n",
        "\n",
        "### 12. More Advanced Seq2Seq Techniques\n",
        "\n",
        "In this section, we will cover some additional aspects of Seq2Seq training and optimization, including **delayed batching**, **gradient clipping**, **bucketing**, **scheduling**, and more.\n",
        "\n",
        "---\n",
        "\n",
        "### 12.1. Delayed Batching\n",
        "\n",
        "**Delayed batching** is a technique used to improve training efficiency by waiting for multiple small batches before processing them together in one large batch. This approach can be useful when training on sequences of varying lengths or when you want to simulate larger batch sizes without needing large memory requirements.\n",
        "\n",
        "In the case of Seq2Seq models, where sequences often have varying lengths, using **dynamic batching** (delayed batching) can save memory and speed up the training process.\n",
        "\n",
        "For example, rather than padding all sequences to the maximum length and then creating a batch, you can collect smaller batches of sequences and then pad them to the maximum length within that batch.\n",
        "\n",
        "#### Example of Delayed Batching:\n",
        "\n",
        "```python\n",
        "# Suppose we have sequences of varying lengths\n",
        "sequences = [\n",
        "    [1, 2, 3],     # Sequence of length 3\n",
        "    [4, 5, 6, 7],  # Sequence of length 4\n",
        "    [8, 9],        # Sequence of length 2\n",
        "    [10, 11, 12, 13, 14],  # Sequence of length 5\n",
        "]\n",
        "\n",
        "# Delayed batching: we collect sequences and then pad them to the maximum length\n",
        "def delayed_batch(sequences, batch_size):\n",
        "    batch = []\n",
        "    max_length = max([len(seq) for seq in sequences])  # Get the max length in this batch\n",
        "    for seq in sequences:\n",
        "        # Pad sequences to the same length in the batch\n",
        "        padded_seq = seq + [0] * (max_length - len(seq))  # Padding with zero\n",
        "        batch.append(padded_seq)\n",
        "    \n",
        "    # Now we have a batch with sequences padded to the same length\n",
        "    return np.array(batch)\n",
        "\n",
        "# Collect a batch\n",
        "batch = delayed_batch(sequences, batch_size=4)\n",
        "print(batch)\n",
        "```\n",
        "\n",
        "In this example:\n",
        "- We wait until we have a few sequences to form a batch.\n",
        "- The sequences are then padded to the same length before forming the batch, which allows more efficient training by utilizing dynamic padding.\n",
        "\n",
        "---\n",
        "\n",
        "### 12.2. Gradient Clipping\n",
        "\n",
        "**Gradient clipping** is a technique used to avoid the problem of **exploding gradients** during training. This is particularly important for Seq2Seq models, as they often involve long sequences where gradients can grow exponentially, leading to unstable training.\n",
        "\n",
        "Gradient clipping works by setting a threshold value for the gradients. If the gradients exceed this threshold, they are scaled down to keep them within the allowed range.\n",
        "\n",
        "#### Example of Gradient Clipping in Keras:\n",
        "\n",
        "In Keras, gradient clipping can be easily applied by specifying the `clipvalue` or `clipnorm` parameter in the optimizer.\n",
        "\n",
        "```python\n",
        "# Using gradient clipping with Keras optimizer\n",
        "optimizer = tf.keras.optimizers.RMSprop(clipvalue=5.0)  # Clip gradients to max value of 5.0\n",
        "\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "- **clipvalue**: This option clips the gradients by value, i.e., if the gradient exceeds this value, it will be scaled down.\n",
        "- **clipnorm**: Clips the gradients by their norm, ensuring that the total magnitude does not exceed a given threshold.\n",
        "\n",
        "Using gradient clipping ensures that the gradients are always within a manageable range, which stabilizes the training process.\n",
        "\n",
        "---\n",
        "\n",
        "### 12.3. Bucketing Sequences by Length\n",
        "\n",
        "**Bucketing** is a technique used to group sequences of similar lengths together into batches. This helps in improving training efficiency by reducing padding. If we were to batch sequences of varying lengths, we would have to pad shorter sequences with zeros, leading to inefficient use of memory.\n",
        "\n",
        "In bucketing, we group sequences into predefined buckets based on their length. Each bucket is padded to the maximum length within that bucket, which reduces unnecessary padding for shorter sequences.\n",
        "\n",
        "#### Example of Bucketing Sequences:\n",
        "\n",
        "```python\n",
        "# Suppose we have the following sequences with varying lengths\n",
        "sequences = [\n",
        "    [1, 2, 3],    # Length 3\n",
        "    [4, 5, 6, 7], # Length 4\n",
        "    [8, 9],       # Length 2\n",
        "    [10, 11, 12, 13, 14], # Length 5\n",
        "]\n",
        "\n",
        "# Define buckets (for example, sequences of length 1-3, 4-5, etc.)\n",
        "buckets = [\n",
        "    (1, 3),  # Sequences of length 1 to 3\n",
        "    (4, 5),  # Sequences of length 4 to 5\n",
        "]\n",
        "\n",
        "# Function to bucket sequences\n",
        "def bucket_sequences(sequences, buckets):\n",
        "    binned_sequences = {bucket: [] for bucket in buckets}\n",
        "    \n",
        "    for seq in sequences:\n",
        "        seq_length = len(seq)\n",
        "        for bucket in buckets:\n",
        "            if bucket[0] <= seq_length <= bucket[1]:\n",
        "                binned_sequences[bucket].append(seq)\n",
        "                break\n",
        "    \n",
        "    return binned_sequences\n",
        "\n",
        "# Bucket the sequences\n",
        "binned_sequences = bucket_sequences(sequences, buckets)\n",
        "print(binned_sequences)\n",
        "```\n",
        "\n",
        "In this example:\n",
        "- Sequences are grouped into buckets based on their length.\n",
        "- The buckets ensure that sequences of similar lengths are processed together, which reduces the need for excessive padding.\n",
        "\n",
        "---\n",
        "\n",
        "### 12.4. Scheduled Sampling\n",
        "\n",
        "**Scheduled Sampling** is a technique that helps in addressing the issue of **exposure bias** in Seq2Seq models. Exposure bias occurs when the model is trained using ground truth tokens (teacher forcing) during training, but during inference, it must generate tokens on its own. This discrepancy can lead to poor generalization.\n",
        "\n",
        "Scheduled Sampling gradually transitions from using the ground truth to using the model's own predictions during training. Initially, the model is trained using the true previous tokens, but as training progresses, it uses more of its own predictions.\n",
        "\n",
        "#### Example of Scheduled Sampling in Keras:\n",
        "\n",
        "Scheduled Sampling can be implemented by modifying the decoder’s input at each timestep:\n",
        "\n",
        "```python\n",
        "import random\n",
        "\n",
        "# Function for scheduled sampling\n",
        "def scheduled_sampling(inputs, outputs, current_step, max_steps):\n",
        "    \"\"\"\n",
        "    Inputs: ground truth sequence\n",
        "    Outputs: decoder predicted sequence\n",
        "    current_step: current step in training\n",
        "    max_steps: total number of steps in the training loop\n",
        "    \"\"\"\n",
        "    use_ground_truth = random.random() < (max_steps - current_step) / max_steps\n",
        "    if use_ground_truth:\n",
        "        return inputs  # Use the ground truth\n",
        "    else:\n",
        "        return outputs  # Use model’s prediction from previous step\n",
        "\n",
        "# Example usage in a training loop\n",
        "for step in range(max_steps):\n",
        "    # Predict using the model (this would typically be done in a training loop)\n",
        "    decoder_input = scheduled_sampling(decoder_input, decoder_output, step, max_steps)\n",
        "    model.train_on_batch([encoder_input, decoder_input], target_output)\n",
        "```\n",
        "\n",
        "In this approach:\n",
        "- **Initially**, the model uses the true labels (ground truth) during training (teacher forcing).\n",
        "- **Gradually**, the model starts using its own predictions during training as the exposure bias is reduced.\n",
        "\n",
        "---\n",
        "\n",
        "### 12.5. Curriculum Learning\n",
        "\n",
        "**Curriculum learning** is a strategy where the model is first trained on simpler tasks or easier sequences before progressing to more complex ones. In the context of Seq2Seq models, this might mean training on shorter sequences before moving on to longer sequences.\n",
        "\n",
        "This method can help the model learn faster and perform better by starting with simpler examples and gradually increasing difficulty.\n",
        "\n",
        "#### Example of Curriculum Learning:\n",
        "\n",
        "```python\n",
        "# Function to implement curriculum learning\n",
        "def curriculum_learning(sequences, length_threshold):\n",
        "    easier_sequences = [seq for seq in sequences if len(seq) <= length_threshold]\n",
        "    harder_sequences = [seq for seq in sequences if len(seq) > length_threshold]\n",
        "    \n",
        "    return easier_sequences, harder_sequences\n",
        "\n",
        "# Curriculum learning example\n",
        "length_threshold = 5\n",
        "easier_sequences, harder_sequences = curriculum_learning(sequences, length_threshold)\n",
        "print(\"Easier sequences:\", easier_sequences)\n",
        "print(\"Harder sequences:\", harder_sequences)\n",
        "```\n",
        "\n",
        "In this method:\n",
        "- The model is trained first on **easier sequences** (shorter sequences) and progressively exposed to more difficult sequences as training progresses.\n",
        "\n",
        "---\n",
        "\n",
        "### 12.6. Masking and Attention for Variable-Length Sequences\n",
        "\n",
        "When working with variable-length sequences, especially when using **attention mechanisms**, it’s crucial to handle padding tokens appropriately. Masking ensures that the padding tokens are ignored during attention calculations.\n",
        "\n",
        "In Keras, you can use the `Masking` layer to automatically mask padded tokens during the training of attention-based models.\n",
        "\n",
        "#### Example of Masking in Keras:\n",
        "\n",
        "```python\n",
        "# Add a masking layer to ignore padding tokens\n",
        "from tensorflow.keras.layers import Masking\n",
        "\n",
        "# Masking layer in the encoder\n",
        "encoder_inputs = layers.Input(shape=(None,))\n",
        "encoder_embedding = layers.Embedding(input_dim=num_encoder_tokens, output_dim=latent_dim)(encoder_inputs)\n",
        "masked_encoder = Masking(mask_value=0)(encoder_embedding)\n",
        "\n",
        "encoder_lstm = layers.LSTM(latent_dim, return_state=True, return_sequences=True)(masked_encoder)\n",
        "encoder_output, state_h, state_c = encoder_lstm\n",
        "```\n",
        "\n",
        "- **Masking** ensures that padding tokens do not contribute to the attention mechanism or loss calculation, leading to more accurate models.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "In this extended explanation, we covered several advanced topics related to Seq2Seq models:\n",
        "\n",
        "- **Delayed Batching**: Efficient batching by waiting for sequences of similar length to form a batch.\n",
        "- **Gradient Clipping**: Preventing exploding gradients by clipping them during training.\n",
        "- **Bucketing Sequences**: Grouping sequences of similar lengths to reduce padding.\n",
        "- **Scheduled Sampling**: Gradually introducing the model's predictions during training to reduce exposure bias.\n",
        "- **Curriculum Learning**: Starting with easier sequences and progressively increasing the difficulty.\n",
        "- **Masking for Attention**: Ensuring padding tokens are ignored during attention-based processing.\n",
        "\n",
        "These techniques are essential for improving the efficiency, stability, and performance of Seq2Seq models, especially in real-world tasks like machine translation and speech recognition. Experimenting with these methods will allow you to build more robust and scalable models."
      ],
      "metadata": {
        "id": "T64E3VOOOAoX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b12D0pQ-OBSn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}